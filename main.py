"""
GlobalID V2 Main Entry Point

ä¸»å…¥å£ï¼šè¿è¡Œå®Œæ•´çš„æ•°æ®çˆ¬å– â†’ åˆ†æ â†’ æŠ¥å‘Šç”Ÿæˆæµç¨‹
"""
import asyncio
import sys
from datetime import datetime, timedelta
from pathlib import Path

import typer
from rich.console import Console
from rich.progress import Progress
from sqlalchemy import select

from src.core import get_config, get_database, get_logger, init_app
from src.domain import Country, Disease, DiseaseRecord, ReportType, CrawlRun
from src.data.crawlers import ChinaCDCCrawler
from src.data.processors import DataProcessor
from src.generation import ReportGenerator

app = typer.Typer(help="GlobalID V2 - Global Infectious Disease Monitoring System")
console = Console()
logger = get_logger(__name__)

@app.command()
def crawl(
    country: str = typer.Option("CN", help="Country code"),
    source: str = typer.Option("all", help="Data source (cdc_weekly/nhc/pubmed/all)"),
    process: bool = typer.Option(True, help="Process and store data"),
    save_raw: bool = typer.Option(True, help="Save raw pages as plain text"),
    force: bool = typer.Option(False, help="Force crawl all data (ignore database check)"),
):
    """
    æ™ºèƒ½çˆ¬å–ç–¾ç—…æ•°æ®
    
    å·¥ä½œæµç¨‹ï¼š
    1. è·å–æ•°æ®åˆ—è¡¨ï¼ˆè½»é‡çº§ï¼‰
    2. ä¸æ•°æ®åº“å¯¹æ¯”ï¼Œè¯†åˆ«æ–°æ•°æ®
    3. åªçˆ¬å–æ–°æ•°æ®çš„è¯¦ç»†å†…å®¹ï¼ˆé‡é‡çº§ï¼‰
    """
    run_id = None
    raw_dir = None

    async def _crawl():
        nonlocal run_id, raw_dir
        await init_app()
        
        # æ ‡å‡†åŒ–å›½å®¶ä»£ç ä¸ºå¤§å†™
        country_code = country.upper()
        
        console.print(f"[bold blue]ğŸš€ Starting intelligent data crawl for {country_code}...[/bold blue]")
        if force:
            console.print("[yellow]âš ï¸  Force mode: will crawl all data (ignoring database)[/yellow]")
        
        # è·å–çˆ¬è™«
        if country_code == "CN":
            crawler = ChinaCDCCrawler()
        else:
            console.print(f"[red]Unsupported country: {country_code}[/red]")
            console.print(f"[yellow]Available countries: CN[/yellow]")
            return
        
        run_id = None
        raw_dir = None
        try:
            raw_dir = Path("data/raw") / country_code.lower()
            async with get_database() as db:
                run = CrawlRun(
                    country_code=country_code,
                    source=source,
                    status="running",
                    started_at=datetime.now(),
                    raw_dir=str(raw_dir) if save_raw else None,
                    metadata_={"force": force, "process": process},
                )
                db.add(run)
                await db.flush()
                run_id = run.id
        except Exception as e:
            logger.warning(f"æ— æ³•åˆ›å»ºçˆ¬å–è¿è¡Œè®°å½•: {e}")

        # æ™ºèƒ½çˆ¬å–ï¼ˆä¸‰é˜¶æ®µï¼‰
        console.print(f"\n[bold cyan]Phase 1/3: Fetching data list...[/bold cyan]")
        results = await crawler.crawl(source=source, force=force)
        
        if not results:
            if run_id:
                async with get_database() as db:
                    run = await db.get(CrawlRun, run_id)
                    if run:
                        run.status = "completed"
                        run.finished_at = datetime.now()
                        run.new_reports = 0
                        run.processed_reports = 0
                        run.total_records = 0
            console.print(f"[yellow]âœ“ No new data to crawl[/yellow]")
            return
        
        console.print(f"[green]âœ“ Found {len(results)} new reports to process[/green]")
        
        # æ˜¾ç¤ºé¢„è§ˆ
        console.print(f"\n[bold]New reports:[/bold]")
        for i, result in enumerate(results[:10], 1):
            date_str = result.date.strftime("%Y-%m") if result.date else "Unknown"
            console.print(f"  {i}. {result.year_month} - {result.title[:80]}...")
        
        if len(results) > 10:
            console.print(f"  ... and {len(results) - 10} more")
        
        # å¤„ç†æ•°æ®
        total_records = 0
        processed = []
        if process and results:
            console.print(f"\n[bold cyan]Phase 2/3: Processing new data...[/bold cyan]")
            
            from src.data.processors import DataProcessor
            
            processor = DataProcessor(
                output_dir=Path("data/processed") / country_code.lower(),
                country_code=country_code.lower()
            )
            
            with Progress() as progress:
                task = progress.add_task("[cyan]Processing...", total=len(results))
                
                processed = await processor.process_crawler_results(
                    results,
                    save_to_file=True,
                    save_raw=save_raw,
                    crawl_run_id=run_id,
                    raw_dir=raw_dir,
                )
                progress.update(task, advance=len(results))
            
            console.print(f"[green]âœ“ Processed {len(processed)}/{len(results)} datasets[/green]")
            
            # ç»Ÿè®¡ä¿¡æ¯
            if processed:
                total_records = sum(len(df) for df in processed)
                console.print(f"[green]âœ“ Total records: {total_records}[/green]")
        
        # ä»…ä¿å­˜åŸå§‹æ–‡æœ¬ï¼ˆä¸å¤„ç†æ•°æ®ï¼‰
        if save_raw and results and not process:
            console.print(f"\n[bold cyan]Phase 3/3: Saving raw data...[/bold cyan]")
            from src.data.processors import DataProcessor

            raw_dir = raw_dir or Path("data/raw") / country_code.lower()
            processor = DataProcessor(
                output_dir=Path("data/processed") / country_code.lower(),
                country_code=country_code.lower(),
            )
            saved = await processor.save_raw_pages(
                results,
                crawl_run_id=run_id,
                raw_dir=raw_dir,
            )
            console.print(f"[green]âœ“ Saved {saved} raw pages to {raw_dir}[/green]")

        if run_id:
            async with get_database() as db:
                run = await db.get(CrawlRun, run_id)
                if run:
                    run.status = "completed"
                    run.finished_at = datetime.now()
                    run.new_reports = len(results)
                    run.processed_reports = len(processed) if process else 0
                    run.total_records = total_records if process and processed else 0
        
        console.print(f"\n[bold green]âœ¨ Crawl completed successfully![/bold green]")
    
    async def _crawl_with_error_handling():
        try:
            await _crawl()
        except Exception as e:
            if run_id:
                async with get_database() as db:
                    run = await db.get(CrawlRun, run_id)
                    if run:
                        run.status = "failed"
                        run.finished_at = datetime.now()
                        run.error_message = str(e)
            raise

    asyncio.run(_crawl_with_error_handling())


@app.command()
def generate_report(
    country: str = typer.Option("CN", help="Country code"),
    report_type: str = typer.Option("weekly", help="Report type (daily/weekly/monthly)"),
    days: int = typer.Option(7, help="Number of days to include"),
    send_email: bool = typer.Option(False, help="Send report via email"),
):
    """
    ç”Ÿæˆç–¾ç—…ç›‘æµ‹æŠ¥å‘Š
    """
    async def _generate():
        await init_app()
        
        console.print(f"[bold blue]Generating {report_type} report for {country}...[/bold blue]")
        
        async with get_database() as db:
            # è·å–å›½å®¶
            country_query = select(Country).where(Country.code == country)
            country_result = await db.execute(country_query)
            country_obj = country_result.scalar_one_or_none()
            
            if not country_obj:
                console.print(f"[red]Country not found: {country}[/red]")
                return
            
            # è®¾ç½®æ—¶é—´èŒƒå›´
            period_end = datetime.now()
            period_start = period_end - timedelta(days=days)
            
            # è·å–æŠ¥å‘Šç±»å‹
            report_type_enum = ReportType[report_type.upper()]
            
            # ç”ŸæˆæŠ¥å‘Š
            generator = ReportGenerator()
            
            with Progress() as progress:
                task = progress.add_task("[cyan]Generating report...", total=100)
                
                report = await generator.generate(
                    country_id=country_obj.id,
                    report_type=report_type_enum,
                    period_start=period_start,
                    period_end=period_end,
                    send_email=send_email,
                )
                
                progress.update(task, advance=100)
            
            console.print(f"[green]âœ“ Report generated successfully![/green]")
            console.print(f"  ID: {report.id}")
            console.print(f"  Status: {report.status}")
            
            if report.markdown_path:
                console.print(f"  Markdown: {report.markdown_path}")
            if report.html_path:
                console.print(f"  HTML: {report.html_path}")
            if report.pdf_path:
                console.print(f"  PDF: {report.pdf_path}")
    
    asyncio.run(_generate())


@app.command()
def init_database():
    """
    åˆå§‹åŒ–æ•°æ®åº“
    """
    async def _init():
        await init_app()
        console.print("[bold blue]Initializing database...[/bold blue]")
        
        from src.domain import Base
        from src.core import get_engine
        
        engine = get_engine()
        
        # åˆ›å»ºæ‰€æœ‰è¡¨
        async with engine.begin() as conn:
            await conn.run_sync(Base.metadata.create_all)
        
        console.print("[green]âœ“ Database initialized[/green]")
        
        # åˆ›å»ºåˆå§‹å›½å®¶æ•°æ®ï¼ˆä¸åˆ›å»ºç–¾ç—…æµ‹è¯•æ•°æ®ï¼‰
        console.print("[blue]Creating initial country data...[/blue]")
        
        # åˆ›å»ºåˆå§‹æ•°æ®ï¼ˆä½¿ç”¨ä¼šè¯ä¸Šä¸‹æ–‡ï¼‰
        async with get_database() as db:
            # åˆ›å»ºä¸­å›½
            country_query = select(Country).where(Country.code == "CN")
            country_result = await db.execute(country_query)
            country = country_result.scalar_one_or_none()

            if not country:
                country = Country(
                    code="CN",
                    name="China",
                    name_en="China",
                    language="zh",
                    timezone="Asia/Shanghai",
                    data_source_url="http://weekly.chinacdc.cn",
                    crawler_config={
                        "sources": ["cdc_weekly", "nhc", "pubmed_rss"],
                    },
                )
                db.add(country)
                await db.commit()
                console.print("  âœ“ Created country: China")
            else:
                console.print("  âœ“ Country China already exists")

            await db.commit()
            console.print("[green]âœ“ Initial country data ready[/green]")
    
    asyncio.run(_init())


@app.command()
def export_data(
    country: str = typer.Option("CN", help="Country code"),
    output_format: str = typer.Option("csv", help="Output format (csv/excel/json/all)"),
    period: str = typer.Option("latest", help="Period (latest/all/YYYY-MM)"),
    package: bool = typer.Option(False, help="Create ZIP package"),
):
    """
    å¯¼å‡ºæ•´ç†å¥½çš„æ•°æ®æ–‡ä»¶
    """
    async def _export():
        await init_app()
        
        from src.generation import DataExporter
        
        console.print(f"[bold blue]Exporting data for {country}...[/bold blue]")
        
        exporter = DataExporter()
        
        # è§£ææ ¼å¼
        if output_format == 'all':
            formats = ['csv', 'excel', 'json']
        else:
            formats = [output_format]
        
        # å¯¼å‡ºæ•°æ®
        if package:
            # åˆ›å»ºæ•°æ®åŒ…
            zip_path = await exporter.create_data_package(
                country_code=country,
                include_all=(period == 'all'),
                include_latest=True,
            )
            console.print(f"[green]âœ“ Data package created: {zip_path}[/green]")
        else:
            if period == 'latest':
                files = await exporter.export_latest(country, formats=formats)
            elif period == 'all':
                files = await exporter.export_all(country, formats=formats)
            else:
                # è§£æYYYY-MM
                try:
                    year, month = map(int, period.split('-'))
                    files = await exporter.export_monthly(country, year, month, formats=formats)
                except:
                    console.print(f"[red]Invalid period format: {period}[/red]")
                    return
            
            console.print(f"[green]âœ“ Exported {len(files)} files:[/green]")
            for fmt, path in files.items():
                console.print(f"  - {fmt.upper()}: {path}")
    
    asyncio.run(_export())


@app.command()
def test():
    """
    è¿è¡Œé›†æˆæµ‹è¯•
    """
    console.print("[bold blue]Running integration tests...[/bold blue]")
    
    # è¿è¡Œæµ‹è¯•
    from tests.test_integration import main as test_main
    
    exit_code = asyncio.run(test_main())
    
    if exit_code == 0:
        console.print("[green]âœ“ All tests passed![/green]")
    else:
        console.print("[red]âœ— Some tests failed[/red]")
    
    sys.exit(exit_code)


@app.command()
def run(
    full: bool = typer.Option(False, help="Run full pipeline (crawl + generate)"),
    force: bool = typer.Option(False, help="Skip data update and use latest available data"),
):
    """
    è¿è¡Œå®Œæ•´æµç¨‹
    """
    async def _run():
        await init_app()
        
        if full:
            console.print("[bold blue]Running full pipeline...[/bold blue]")
            
            # 1. çˆ¬å–æ•°æ®ï¼ˆé™¤éforce=Trueï¼‰
            period_start = None
            period_end = None
            
            if not force:
                console.print("\n[cyan]Step 1: Crawling data[/cyan]")
                period_start, period_end = await _crawl()
            else:
                console.print("\n[yellow]Step 1: Skipping data crawl (force mode)[/yellow]")
                # è·å–æ•°æ®åº“ä¸­æœ€æ–°çš„æ•°æ®æ—¶é—´
                period_start, period_end = await _get_latest_data_period()
                if period_start and period_end:
                    console.print(f"  Using latest data period: {period_start.date()} to {period_end.date()}")
                else:
                    console.print("[red]No data found in database. Please run without --force first.[/red]")
                    return
            
            # 2. ç”ŸæˆæŠ¥å‘Šï¼ˆåŸºäºçˆ¬å–åˆ°çš„æ•°æ®æ—¶é—´èŒƒå›´ï¼‰
            console.print("\n[cyan]Step 2: Generating report[/cyan]")
            await _generate(period_start, period_end)
            
            console.print("\n[green]âœ“ Pipeline completed![/green]")
        else:
            console.print("[yellow]Use --full to run the complete pipeline[/yellow]")
    
    async def _get_latest_data_period():
        """ä»æ•°æ®åº“è·å–æœ€æ–°çš„æ•°æ®æ—¶é—´èŒƒå›´"""
        from sqlalchemy import text
        async with get_database() as db:
            result = await db.execute(text("""
                SELECT MIN(time) as min_time, MAX(time) as max_time
                FROM disease_records
                WHERE country_id = (SELECT id FROM countries WHERE code = 'CN')
            """))
            row = result.fetchone()
            if row and row[0] and row[1]:
                return row[0], row[1]
        return None, None
    
    async def _crawl():
        """çˆ¬å–æ–°æ•°æ®ã€å¤„ç†å¹¶ä¿å­˜åˆ°æ•°æ®åº“ï¼Œè¿”å›æ•°æ®æ—¶é—´èŒƒå›´"""
        from src.data.processors import DataProcessor
        
        crawler = ChinaCDCCrawler()
        # First try normal crawl
        results = await crawler.crawl(source="all", force=False)
        
        # If no new data found, force crawl for pipeline testing
        if not results:
            console.print("   No new data found, using force mode for testing...")
            results = await crawler.crawl(source="all", force=True)
        
        console.print(f"  Fetched {len(results)} results")
        
        # å¤„ç†æ•°æ®å¹¶ä¿å­˜åˆ°æ•°æ®åº“
        if results:
            processor = DataProcessor(
                output_dir=Path("data/processed") / "cn",
                country_code="cn"
            )
            
            processed = await processor.process_crawler_results(
                results,
                save_to_file=True,
                save_raw=True,
                crawl_run_id=None,  # No need for crawl run tracking in auto mode
                raw_dir=Path("data/raw/cn"),
            )
            console.print(f"  Processed {len(processed)} datasets with {sum(len(df) for df in processed)} total records")
        
        # æå–æ•°æ®æ—¶é—´èŒƒå›´
        dates = [r.date for r in results if r.date]
        if dates:
            min_date = min(dates)
            max_date = max(dates)
            return min_date, max_date
        return None, None
    
    async def _generate(period_start=None, period_end=None):
        """ç”ŸæˆæŠ¥å‘Šï¼Œå¦‚æœæ²¡æœ‰æŒ‡å®šæ—¶é—´èŒƒå›´ï¼Œåˆ™ä½¿ç”¨æœ€è¿‘90å¤©"""
        async with get_database() as db:
            country_query = select(Country).where(Country.code == "CN")
            country_result = await db.execute(country_query)
            country = country_result.scalar_one()

            # å¦‚æœæ²¡æœ‰æŒ‡å®šæ—¶é—´èŒƒå›´ï¼Œä½¿ç”¨æœ€è¿‘90å¤©çš„æ•°æ®
            if period_start is None or period_end is None:
                period_end = datetime.now()
                period_start = period_end - timedelta(days=90)
                console.print(f"  Using default time range: last 90 days")
            else:
                console.print(f"  Using data time range: {period_start.date()} to {period_end.date()}")

            generator = ReportGenerator()
            report = await generator.generate(
                country_id=country.id,
                report_type=ReportType.WEEKLY,
                period_start=period_start,
                period_end=period_end,
            )

            console.print(f"  Report generated: {report.id}")
    
    asyncio.run(_run())


if __name__ == "__main__":
    app()
