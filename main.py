"""
GlobalID V2 Main Entry Point

ä¸»å…¥å£ï¼šè¿è¡Œå®Œæ•´çš„æ•°æ®çˆ¬å– â†’ åˆ†æ â†’ æŠ¥å‘Šç”Ÿæˆæµç¨‹
"""
import asyncio
import signal
import sys
from datetime import datetime, timedelta
from pathlib import Path

import typer
from rich.console import Console
from rich.progress import Progress
from sqlalchemy import select

from src.core import get_config, get_database, get_logger, init_app
from src.core.task_manager import task_manager
from src.domain import Country, Disease, DiseaseRecord, ReportType, CrawlRun, TaskType, TaskPriority, TaskStatus, Task
from src.data.crawlers import ChinaCDCCrawler
from src.data.processors import DataProcessor
from src.generation import ReportGenerator

app = typer.Typer(help="GlobalID V2 - Global Infectious Disease Monitoring System")
console = Console()
logger = get_logger(__name__)

@app.command()
def crawl(
    country: str = typer.Option("CN", help="Country code"),
    source: str = typer.Option("all", help="Data source (cdc_weekly/nhc/pubmed/all)"),
    process: bool = typer.Option(True, help="Process and store data"),
    save_raw: bool = typer.Option(True, help="Save raw pages as plain text"),
    force: bool = typer.Option(False, help="Force crawl all data (ignore database check)"),
):
    """
    æ™ºèƒ½çˆ¬å–ç–¾ç—…æ•°æ®
    
    å·¥ä½œæµç¨‹ï¼š
    1. è·å–æ•°æ®åˆ—è¡¨ï¼ˆè½»é‡çº§ï¼‰
    2. ä¸æ•°æ®åº“å¯¹æ¯”ï¼Œè¯†åˆ«æ–°æ•°æ®
    3. åªçˆ¬å–æ–°æ•°æ®çš„è¯¦ç»†å†…å®¹ï¼ˆé‡é‡çº§ï¼‰
    """
    run_id = None
    raw_dir = None
    task = None

    async def _crawl():
        nonlocal run_id, raw_dir, task
        await init_app()
        
        # æ ‡å‡†åŒ–å›½å®¶ä»£ç ä¸ºå¤§å†™
        country_code = country.upper()
        
        # Create task record
        task_name = f"Crawl {country_code} Data ({source})"
        task_description = f"Source: {source}, Force: {'Yes' if force else 'No'}, Process: {'Yes' if process else 'No'}"
        
        task = await task_manager.create_task(
            task_type=TaskType.CRAWL_DATA,
            task_name=task_name,
            priority=TaskPriority.HIGH if force else TaskPriority.NORMAL,
            description=task_description,
            input_data={
                "country": country_code,
                "source": source,
                "force": force,
                "process": process,
                "save_raw": save_raw,
            }
        )
        
        console.print(f"[bold blue]ğŸš€ Starting intelligent data crawl for {country_code}...[/bold blue]")
        console.print(f"[dim]Task UUID: {task.task_uuid}[/dim]")
        if force:
            console.print("[yellow]âš ï¸  Force mode: will crawl all data (ignoring database)[/yellow]")
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­
        await task_manager.update_task_status(task.task_uuid, TaskStatus.RUNNING)
        
        # Add startup log
        await task_manager.add_workbook_entry(
            task.task_uuid,
            entry_type="info",
            title="Crawl Task Started",
            content=f"Country: {country_code}\nSource: {source}\nForce Mode: {force}\nProcess Data: {process}",
            content_type="text"
        )
        
        # è·å–çˆ¬è™«
        if country_code == "CN":
            crawler = ChinaCDCCrawler()
        else:
            console.print(f"[red]Unsupported country: {country_code}[/red]")
            console.print(f"[yellow]Available countries: CN[/yellow]")
            await task_manager.update_task_status(
                task.task_uuid,
                TaskStatus.FAILED,
                error_message=f"Unsupported country: {country_code}"
            )
            return
        
        run_id = None
        raw_dir = None
        try:
            raw_dir = Path("data/raw") / country_code.lower()
            async with get_database() as db:
                run = CrawlRun(
                    country_code=country_code,
                    source=source,
                    status="running",
                    started_at=datetime.now(),
                    raw_dir=str(raw_dir) if save_raw else None,
                    metadata_={"force": force, "process": process},
                )
                db.add(run)
                await db.flush()
                run_id = run.id
        except Exception as e:
            logger.warning(f"æ— æ³•åˆ›å»ºçˆ¬å–è¿è¡Œè®°å½•: {e}")

        # æ™ºèƒ½çˆ¬å–ï¼ˆä¸‰é˜¶æ®µï¼‰
        console.print(f"\n[bold cyan]Phase 1/3: Fetching data list...[/bold cyan]")
        
        # Log phase 1
        await task_manager.add_workbook_entry(
            task.task_uuid,
            entry_type="info",
            title="Phase 1/3: Fetching Data List",
            content="Starting to fetch available data list...",
            content_type="text"
        )
        
        # Update progress: 10%
        await task_manager.update_task_progress(task.task_uuid, 1, 10)
        
        results = await crawler.crawl(source=source, force=force)
        
        # Update progress: 30%
        await task_manager.update_task_progress(task.task_uuid, 3, 10)
        
        if not results:
            if run_id:
                async with get_database() as db:
                    run = await db.get(CrawlRun, run_id)
                    if run:
                        run.status = "completed"
                        run.finished_at = datetime.now()
                        run.new_reports = 0
                        run.processed_reports = 0
                        run.total_records = 0
            console.print(f"[yellow]âœ“ No new data to crawl[/yellow]")
            
            # Log no new data
            await task_manager.add_workbook_entry(
                task.task_uuid,
                entry_type="info",
                title="Crawl Completed",
                content="No new data found to crawl",
                content_type="text"
            )
            
            # æ›´æ–°ä»»åŠ¡è¾“å‡ºæ•°æ®
            async with get_database() as db:
                task_obj = await db.get(Task, task.id)
                if task_obj:
                    task_obj.output_data = {"message": "No new data to crawl"}
                    await db.commit()
            
            # Update progress to 100%
            await task_manager.update_task_progress(task.task_uuid, 10, 10)
            
            await task_manager.update_task_status(
                task.task_uuid,
                TaskStatus.COMPLETED
            )
            return
        
        console.print(f"[green]âœ“ Found {len(results)} new reports to process[/green]")
        
        # Log discovered data
        await task_manager.add_workbook_entry(
            task.task_uuid,
            entry_type="success",
            title="Data List Retrieved",
            content=f"Found {len(results)} {'new ' if not force else ''}report(s) to process",
            content_type="text"
        )
        
        # Update progress: 40%
        await task_manager.update_task_progress(task.task_uuid, 4, 10)
        
        # æ˜¾ç¤ºé¢„è§ˆ
        console.print(f"\n[bold]New reports:[/bold]")
        for i, result in enumerate(results[:10], 1):
            date_str = result.date.strftime("%Y-%m") if result.date else "Unknown"
            console.print(f"  {i}. {result.year_month} - {result.title[:80]}...")
        
        if len(results) > 10:
            console.print(f"  ... and {len(results) - 10} more")
        
        # å¤„ç†æ•°æ®
        total_records = 0
        processed = []
        if process and results:
            console.print(f"\n[bold cyan]Phase 2/3: Processing new data...[/bold cyan]")
            
            # Log phase 2
            await task_manager.add_workbook_entry(
                task.task_uuid,
                entry_type="info",
                title="Phase 2/3: Processing Data",
                content=f"Starting to process {len(results)} report(s)...",
                content_type="text"
            )
            
            # Update progress: 50%
            await task_manager.update_task_progress(task.task_uuid, 5, 10)
            
            from src.data.processors import DataProcessor
            
            processor = DataProcessor(
                output_dir=Path("data/processed") / country_code.lower(),
                country_code=country_code.lower()
            )
            
            # Progress callback for workbook logging
            processed_count = 0
            async def progress_callback(current, total, message):
                nonlocal processed_count
                processed_count = current
                # Update progress between 50% and 80%
                progress_pct = 50 + int((current / total) * 30)
                await task_manager.update_task_progress(task.task_uuid, progress_pct // 10, 10)
                
                # Log every 10 reports or at key milestones
                if current % 10 == 0 or current == total:
                    await task_manager.add_workbook_entry(
                        task.task_uuid,
                        entry_type="info",
                        title="Processing Progress",
                        content=f"{current}/{total} reports processed",
                        content_type="text"
                    )
            
            with Progress() as progress:
                progress_task = progress.add_task("[cyan]Processing...", total=len(results))
                
                processed = await processor.process_crawler_results(
                    results,
                    save_to_file=True,
                    save_raw=save_raw,
                    crawl_run_id=run_id,
                    raw_dir=raw_dir,
                    progress_callback=progress_callback,
                )
                progress.update(progress_task, advance=len(results))
            
            console.print(f"[green]âœ“ Processed {len(processed)}/{len(results)} datasets[/green]")
            
            # ç»Ÿè®¡ä¿¡æ¯
            if processed:
                total_records = sum(len(df) for df in processed)
                console.print(f"[green]âœ“ Total records: {total_records}[/green]")
                
                # Log processing result
                await task_manager.add_workbook_entry(
                    task.task_uuid,
                    entry_type="success",
                    title="Data Processing Completed",
                    content=f"Successfully processed: {len(processed)}/{len(results)} dataset(s)\nTotal records: {total_records}",
                    content_type="text"
                )
                
                # Update progress: 80%
                await task_manager.update_task_progress(task.task_uuid, 8, 10)
        
        # ä»…ä¿å­˜åŸå§‹æ–‡æœ¬ï¼ˆä¸å¤„ç†æ•°æ®ï¼‰
        if save_raw and results and not process:
            console.print(f"\n[bold cyan]Phase 3/3: Saving raw data...[/bold cyan]")
            from src.data.processors import DataProcessor

            raw_dir = raw_dir or Path("data/raw") / country_code.lower()
            processor = DataProcessor(
                output_dir=Path("data/processed") / country_code.lower(),
                country_code=country_code.lower(),
            )
            saved = await processor.save_raw_pages(
                results,
                crawl_run_id=run_id,
                raw_dir=raw_dir,
            )
            console.print(f"[green]âœ“ Saved {saved} raw pages to {raw_dir}[/green]")

        if run_id:
            async with get_database() as db:
                run = await db.get(CrawlRun, run_id)
                if run:
                    run.status = "completed"
                    run.finished_at = datetime.now()
                    run.new_reports = len(results)
                    run.processed_reports = len(processed) if process else 0
                    run.total_records = total_records if process and processed else 0
        
        # Log completion
        await task_manager.add_workbook_entry(
            task.task_uuid,
            entry_type="success",
            title="Crawl Task Completed",
            content=f"New reports: {len(results)}\nProcessed datasets: {len(processed) if process else 0}\nTotal records: {total_records if process and processed else 0}",
            content_type="text"
        )
        
        # æ›´æ–°ä»»åŠ¡è¾“å‡ºæ•°æ®å’ŒçŠ¶æ€
        async with get_database() as db:
            task_obj = await db.get(Task, task.id)
            if task_obj:
                task_obj.output_data = {
                    "new_reports": len(results),
                    "processed_reports": len(processed) if process else 0,
                    "total_records": total_records if process and processed else 0,
                    "crawl_run_id": run_id,
                }
                await db.commit()
        
        # Update progress to 100%
        await task_manager.update_task_progress(task.task_uuid, 10, 10)
        
        await task_manager.update_task_status(
            task.task_uuid,
            TaskStatus.COMPLETED
        )
        
        console.print(f"\n[bold green]âœ¨ Crawl completed successfully![/bold green]")
    
    # Flag to track if task was cancelled
    task_cancelled = False
    
    def signal_handler(signum, frame):
        """Handle SIGINT (Ctrl+C) and SIGTERM signals"""
        nonlocal task_cancelled
        task_cancelled = True
        console.print("\n[yellow]âš ï¸  Cancellation requested... cleaning up...[/yellow]")
    
    # Register signal handlers
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    async def _crawl_with_error_handling():
        try:
            await _crawl()
        except KeyboardInterrupt:
            # Handle Ctrl+C interruption
            console.print("\n[yellow]âš ï¸  Task interrupted by user[/yellow]")
            
            # Update CrawlRun status
            if run_id:
                async with get_database() as db:
                    run = await db.get(CrawlRun, run_id)
                    if run:
                        run.status = "cancelled"
                        run.finished_at = datetime.now()
                        run.error_message = "Interrupted by user (Ctrl+C)"
                        await db.commit()
            
            # Update task status to CANCELLED
            if task:
                # Add cancellation log
                await task_manager.add_workbook_entry(
                    task.task_uuid,
                    entry_type="warning",
                    title="Task Cancelled",
                    content="Task was interrupted by user (Ctrl+C)",
                    content_type="text"
                )
                
                await task_manager.update_task_status(
                    task.task_uuid,
                    TaskStatus.CANCELLED,
                    error_message="Interrupted by user"
                )
            
            console.print("[yellow]âœ“ Task cancelled and status updated[/yellow]")
            sys.exit(130)  # Standard exit code for SIGINT
            
        except Exception as e:
            # Handle other errors
            # Update CrawlRun status
            if run_id:
                async with get_database() as db:
                    run = await db.get(CrawlRun, run_id)
                    if run:
                        run.status = "failed"
                        run.finished_at = datetime.now()
                        run.error_message = str(e)
                        await db.commit()
            
            # Update task status to FAILED
            if task:
                # Add error log
                await task_manager.add_workbook_entry(
                    task.task_uuid,
                    entry_type="error",
                    title="Task Failed",
                    content=f"Error: {str(e)}",
                    content_type="text"
                )
                
                await task_manager.update_task_status(
                    task.task_uuid,
                    TaskStatus.FAILED,
                    error_message=str(e)
                )
            
            logger.error(f"Crawl failed: {e}")
            console.print(f"[red]âœ— Task failed: {e}[/red]")
            raise

    asyncio.run(_crawl_with_error_handling())


@app.command()
def generate_report(
    country: str = typer.Option("CN", help="Country code"),
    report_type: str = typer.Option("weekly", help="Report type (daily/weekly/monthly)"),
    days: int = typer.Option(7, help="Number of days to include"),
    send_email: bool = typer.Option(False, help="Send report via email"),
):
    """
    ç”Ÿæˆç–¾ç—…ç›‘æµ‹æŠ¥å‘Š
    """
    async def _generate():
        await init_app()
        
        console.print(f"[bold blue]Generating {report_type} report for {country}...[/bold blue]")
        
        async with get_database() as db:
            # è·å–å›½å®¶
            country_query = select(Country).where(Country.code == country)
            country_result = await db.execute(country_query)
            country_obj = country_result.scalar_one_or_none()
            
            if not country_obj:
                console.print(f"[red]Country not found: {country}[/red]")
                return
            
            # è®¾ç½®æ—¶é—´èŒƒå›´
            period_end = datetime.now()
            period_start = period_end - timedelta(days=days)
            
            # è·å–æŠ¥å‘Šç±»å‹
            report_type_enum = ReportType[report_type.upper()]
            
            # ç”ŸæˆæŠ¥å‘Š
            generator = ReportGenerator()
            
            with Progress() as progress:
                task = progress.add_task("[cyan]Generating report...", total=100)
                
                report = await generator.generate(
                    country_id=country_obj.id,
                    report_type=report_type_enum,
                    period_start=period_start,
                    period_end=period_end,
                    send_email=send_email,
                )
                
                progress.update(task, advance=100)
            
            console.print(f"[green]âœ“ Report generated successfully![/green]")
            console.print(f"  ID: {report.id}")
            console.print(f"  Status: {report.status}")
            
            if report.markdown_path:
                console.print(f"  Markdown: {report.markdown_path}")
            if report.html_path:
                console.print(f"  HTML: {report.html_path}")
            if report.pdf_path:
                console.print(f"  PDF: {report.pdf_path}")
    
    asyncio.run(_generate())


@app.command()
def init_database():
    """
    åˆå§‹åŒ–æ•°æ®åº“
    """
    async def _init():
        await init_app()
        console.print("[bold blue]Initializing database...[/bold blue]")
        
        from src.domain import Base
        from src.core import get_engine
        
        engine = get_engine()
        
        # åˆ›å»ºæ‰€æœ‰è¡¨
        async with engine.begin() as conn:
            await conn.run_sync(Base.metadata.create_all)
        
        console.print("[green]âœ“ Database initialized[/green]")
        
        # åˆ›å»ºåˆå§‹å›½å®¶æ•°æ®ï¼ˆä¸åˆ›å»ºç–¾ç—…æµ‹è¯•æ•°æ®ï¼‰
        console.print("[blue]Creating initial country data...[/blue]")
        
        # åˆ›å»ºåˆå§‹æ•°æ®ï¼ˆä½¿ç”¨ä¼šè¯ä¸Šä¸‹æ–‡ï¼‰
        async with get_database() as db:
            # åˆ›å»ºä¸­å›½
            country_query = select(Country).where(Country.code == "CN")
            country_result = await db.execute(country_query)
            country = country_result.scalar_one_or_none()

            if not country:
                country = Country(
                    code="CN",
                    name="China",
                    name_en="China",
                    language="zh",
                    timezone="Asia/Shanghai",
                    data_source_url="http://weekly.chinacdc.cn",
                    crawler_config={
                        "sources": ["cdc_weekly", "nhc", "pubmed_rss"],
                    },
                )
                db.add(country)
                await db.commit()
                console.print("  âœ“ Created country: China")
            else:
                console.print("  âœ“ Country China already exists")

            await db.commit()
            console.print("[green]âœ“ Initial country data ready[/green]")
    
    asyncio.run(_init())


@app.command()
def export_data(
    country: str = typer.Option("CN", help="Country code"),
    output_format: str = typer.Option("csv", help="Output format (csv/excel/json/all)"),
    period: str = typer.Option("latest", help="Period (latest/all/YYYY-MM)"),
    package: bool = typer.Option(False, help="Create ZIP package"),
):
    """
    å¯¼å‡ºæ•´ç†å¥½çš„æ•°æ®æ–‡ä»¶
    """
    async def _export():
        await init_app()
        
        from src.generation import DataExporter
        
        console.print(f"[bold blue]Exporting data for {country}...[/bold blue]")
        
        exporter = DataExporter()
        
        # è§£ææ ¼å¼
        if output_format == 'all':
            formats = ['csv', 'excel', 'json']
        else:
            formats = [output_format]
        
        # å¯¼å‡ºæ•°æ®
        if package:
            # åˆ›å»ºæ•°æ®åŒ…
            zip_path = await exporter.create_data_package(
                country_code=country,
                include_all=(period == 'all'),
                include_latest=True,
            )
            console.print(f"[green]âœ“ Data package created: {zip_path}[/green]")
        else:
            if period == 'latest':
                files = await exporter.export_latest(country, formats=formats)
            elif period == 'all':
                files = await exporter.export_all(country, formats=formats)
            else:
                # è§£æYYYY-MM
                try:
                    year, month = map(int, period.split('-'))
                    files = await exporter.export_monthly(country, year, month, formats=formats)
                except:
                    console.print(f"[red]Invalid period format: {period}[/red]")
                    return
            
            console.print(f"[green]âœ“ Exported {len(files)} files:[/green]")
            for fmt, path in files.items():
                console.print(f"  - {fmt.upper()}: {path}")
    
    asyncio.run(_export())


@app.command()
def test():
    """
    è¿è¡Œé›†æˆæµ‹è¯•
    """
    console.print("[bold blue]Running integration tests...[/bold blue]")
    
    # è¿è¡Œæµ‹è¯•
    from tests.test_integration import main as test_main
    
    exit_code = asyncio.run(test_main())
    
    if exit_code == 0:
        console.print("[green]âœ“ All tests passed![/green]")
    else:
        console.print("[red]âœ— Some tests failed[/red]")
    
    sys.exit(exit_code)


@app.command()
def run(
    full: bool = typer.Option(False, help="Run full pipeline (crawl + generate)"),
    force: bool = typer.Option(False, help="Skip data update and use latest available data"),
):
    """
    è¿è¡Œå®Œæ•´æµç¨‹
    """
    async def _run():
        await init_app()
        
        if full:
            console.print("[bold blue]Running full pipeline...[/bold blue]")
            
            # 1. çˆ¬å–æ•°æ®ï¼ˆé™¤éforce=Trueï¼‰
            period_start = None
            period_end = None
            
            if not force:
                console.print("\n[cyan]Step 1: Crawling data[/cyan]")
                period_start, period_end = await _crawl()
            else:
                console.print("\n[yellow]Step 1: Skipping data crawl (force mode)[/yellow]")
                # è·å–æ•°æ®åº“ä¸­æœ€æ–°çš„æ•°æ®æ—¶é—´
                period_start, period_end = await _get_latest_data_period()
                if period_start and period_end:
                    console.print(f"  Using latest data period: {period_start.date()} to {period_end.date()}")
                else:
                    console.print("[red]No data found in database. Please run without --force first.[/red]")
                    return
            
            # 2. ç”ŸæˆæŠ¥å‘Šï¼ˆåŸºäºçˆ¬å–åˆ°çš„æ•°æ®æ—¶é—´èŒƒå›´ï¼‰
            console.print("\n[cyan]Step 2: Generating report[/cyan]")
            await _generate(period_start, period_end)
            
            console.print("\n[green]âœ“ Pipeline completed![/green]")
        else:
            console.print("[yellow]Use --full to run the complete pipeline[/yellow]")
    
    async def _get_latest_data_period():
        """ä»æ•°æ®åº“è·å–æœ€æ–°çš„æ•°æ®æ—¶é—´èŒƒå›´"""
        from sqlalchemy import text
        async with get_database() as db:
            result = await db.execute(text("""
                SELECT MIN(time) as min_time, MAX(time) as max_time
                FROM disease_records
                WHERE country_id = (SELECT id FROM countries WHERE code = 'CN')
            """))
            row = result.fetchone()
            if row and row[0] and row[1]:
                return row[0], row[1]
        return None, None
    
    async def _crawl():
        """çˆ¬å–æ–°æ•°æ®ã€å¤„ç†å¹¶ä¿å­˜åˆ°æ•°æ®åº“ï¼Œè¿”å›æ•°æ®æ—¶é—´èŒƒå›´"""
        from src.data.processors import DataProcessor
        
        crawler = ChinaCDCCrawler()
        # First try normal crawl
        results = await crawler.crawl(source="all", force=False)
        
        # If no new data found, force crawl for pipeline testing
        if not results:
            console.print("   No new data found, using force mode for testing...")
            results = await crawler.crawl(source="all", force=True)
        
        console.print(f"  Fetched {len(results)} results")
        
        # å¤„ç†æ•°æ®å¹¶ä¿å­˜åˆ°æ•°æ®åº“
        if results:
            processor = DataProcessor(
                output_dir=Path("data/processed") / "cn",
                country_code="cn"
            )
            
            processed = await processor.process_crawler_results(
                results,
                save_to_file=True,
                save_raw=True,
                crawl_run_id=None,  # No need for crawl run tracking in auto mode
                raw_dir=Path("data/raw/cn"),
            )
            console.print(f"  Processed {len(processed)} datasets with {sum(len(df) for df in processed)} total records")
        
        # æå–æ•°æ®æ—¶é—´èŒƒå›´
        dates = [r.date for r in results if r.date]
        if dates:
            min_date = min(dates)
            max_date = max(dates)
            return min_date, max_date
        return None, None
    
    async def _generate(period_start=None, period_end=None):
        """ç”ŸæˆæŠ¥å‘Šï¼Œå¦‚æœæ²¡æœ‰æŒ‡å®šæ—¶é—´èŒƒå›´ï¼Œåˆ™ä½¿ç”¨æœ€è¿‘90å¤©"""
        async with get_database() as db:
            country_query = select(Country).where(Country.code == "CN")
            country_result = await db.execute(country_query)
            country = country_result.scalar_one()

            # å¦‚æœæ²¡æœ‰æŒ‡å®šæ—¶é—´èŒƒå›´ï¼Œä½¿ç”¨æœ€è¿‘90å¤©çš„æ•°æ®
            if period_start is None or period_end is None:
                period_end = datetime.now()
                period_start = period_end - timedelta(days=90)
                console.print(f"  Using default time range: last 90 days")
            else:
                console.print(f"  Using data time range: {period_start.date()} to {period_end.date()}")

            generator = ReportGenerator()
            report = await generator.generate(
                country_id=country.id,
                report_type=ReportType.WEEKLY,
                period_start=period_start,
                period_end=period_end,
            )

            console.print(f"  Report generated: {report.id}")
    
    asyncio.run(_run())


if __name__ == "__main__":
    app()
